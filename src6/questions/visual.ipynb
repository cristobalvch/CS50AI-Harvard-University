{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "265f6c375f5723599e793514545a17c1828385c656a37de44dc67ad3be04634e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_MATCHES = 1\n",
    "SENTENCE_MATCHES = 1\n",
    "\n",
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    corpus_dict = dict()\n",
    "    filenames = os.listdir(directory)\n",
    "    for file in filenames:\n",
    "        path = os.path.join('corpus',file)\n",
    "        with open(path,encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            corpus_dict[file] = text     \n",
    "\n",
    "    return corpus_dict\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Given a document (represented as a string), return a list of all of the\n",
    "    words in that document, in order.\n",
    "\n",
    "    Process document by coverting all words to lowercase, and removing any\n",
    "    punctuation or English stopwords.\n",
    "    \"\"\"\n",
    "    document = document.lower()\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    stop_words  = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(document)\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "def compute_idfs(documents):\n",
    "    \"\"\"\n",
    "    Given a dictionary of `documents` that maps names of documents to a list\n",
    "    of words, return a dictionary that maps words to their IDF values.\n",
    "\n",
    "    Any word that appears in at least one of the documents should be in the\n",
    "    resulting dictionary.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    for file in documents:\n",
    "        words.update(documents[file])\n",
    "\n",
    "    idfs = dict()\n",
    "    for word in words:\n",
    "        tw = sum(word in documents[file] for file in documents)\n",
    "        idf = math.log(len(documents)/ tw )\n",
    "        idfs[word] = idf\n",
    "\n",
    "    return idfs\n",
    "\n",
    "def top_files(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
    "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the filenames of the the `n` top\n",
    "    files that match the query, ranked according to tf-idf.\n",
    "    \"\"\"\n",
    "    tfidfs = dict()\n",
    "    for filename in files:\n",
    "        tfidfs[filename] = 0\n",
    "        for word in query:\n",
    "             tfidfs[filename] += files[filename].count(word) * idfs[word]\n",
    "\n",
    "    files_idfs = sorted(tfidfs.items(), key=lambda item: item[1], reverse=True)[:n]\n",
    "\n",
    "    return [key for key, value in files_idfs]\n",
    "\n",
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
    "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the `n` top sentences that match\n",
    "    the query, ranked according to idf. If there are ties, preference should\n",
    "    be given to sentences that have a higher query term density.\n",
    "    \"\"\"\n",
    "    ranking = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        values = [sentence,0,0]\n",
    "\n",
    "    for word in query:\n",
    "        if word in sentences[sentence]:\n",
    "            #tf-idf\n",
    "            values[1] += idfs[word]\n",
    "            #query term density\n",
    "            values[2] += sentences[sentence].count(word) / len(sentences[sentence])\n",
    "    \n",
    "    ranking.append(values)\n",
    "    top_sentences = [sentence for sentence, mwm, qtd in sorted(ranking, \n",
    "                                                     key=lambda item: (item[1], item[2]), reverse=True)][:n]\n",
    "\n",
    "\n",
    "    return top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = load_files('corpus')\n",
    "file_words = {\n",
    "        filename: tokenize(files[filename])\n",
    "        for filename in files\n",
    "    }\n",
    "file_idfs = compute_idfs(file_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_MATCHES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = set(tokenize('When was python 3.0 released?'))\n",
    "\n",
    "filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "sentences = dict()\n",
    "for filename in filenames:\n",
    "    for passage in files[filename].split(\"\\n\"):\n",
    "        for sentence in nltk.sent_tokenize(passage):\n",
    "            tokens = tokenize(sentence)\n",
    "            if tokens:\n",
    "                sentences[sentence] = tokens\n",
    "\n",
    "sentences_idfs = compute_idfs(sentences)\n",
    "\n",
    "matches = top_sentences(query, sentences, sentences_idfs, n=SENTENCE_MATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['python.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}